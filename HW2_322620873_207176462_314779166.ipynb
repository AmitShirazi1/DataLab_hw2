{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46a6e2b4-819e-484e-ace4-4f87c517baee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Read data into PySpark Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae3c010e-3876-4dbd-87bf-eb294b63f8d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "548ea066-c251-47cc-a6f6-45541f4c903c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "companies = spark.read.parquet('/dbfs/linkedin_train_data')\n",
    "profiles = spark.read.parquet('/dbfs/linkedin_people_train_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "643e9b98-c5d3-466b-8423-2a3f4d8f0a7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "meta_industries_12 = {\n",
    "    'Furniture and Home Furnishings Manufacturing': 'Manufacturing',\n",
    "    'Investment Banking': 'Financial and Investment',\n",
    "    'Architecture and Planning': 'Services',\n",
    "    'Wholesale': 'Services',\n",
    "    'Travel Arrangements': 'Services',\n",
    "    'Ranching': 'Miscellaneous',\n",
    "    'Hospitals and Health Care': 'Healthcare and Medical',\n",
    "    'Book and Periodical Publishing': 'Services',\n",
    "    'Printing Services': 'Services',\n",
    "    'Professional Training and Coaching': 'Services',\n",
    "    'Computers and Electronics Manufacturing': 'Manufacturing',\n",
    "    'Shipbuilding': 'Manufacturing',\n",
    "    'Public Policy Offices': 'Government and Public Policy',\n",
    "    'Software Development': 'Technology',\n",
    "    'Outsourcing and Offshoring Consulting': 'Services',\n",
    "    'Retail Groceries': 'Retail and Consumer Goods',\n",
    "    'Education Administration Programs': 'Education and Training',\n",
    "    'Plastics Manufacturing': 'Manufacturing',\n",
    "    'Renewable Energy Semiconductor Manufacturing': 'Manufacturing',\n",
    "    'Computer Networking Products': 'Technology',\n",
    "    'Events Services': 'Services',\n",
    "    'Information Services': 'Services',\n",
    "    'Food and Beverage Services': 'Services',\n",
    "    'Semiconductor Manufacturing': 'Manufacturing',\n",
    "    'Business Consulting and Services': 'Services',\n",
    "    'Insurance': 'Services',\n",
    "    'Financial Services': 'Services',\n",
    "    'Wireless Services': 'Services',\n",
    "    'Computer Hardware Manufacturing': 'Technology',\n",
    "    'Public Safety': 'Services',\n",
    "    'Maritime Transportation': 'Transportation and Logistics',\n",
    "    'Tobacco Manufacturing': 'Manufacturing',\n",
    "    'Writing and Editing': 'Services',\n",
    "    'Veterinary Services': 'Services',\n",
    "    'Staffing and Recruiting': 'Services',\n",
    "    'Accounting': 'Services',\n",
    "    'International Affairs': 'Government and Public Policy',\n",
    "    'Spectator Sports': 'Miscellaneous',\n",
    "    'Glass, Ceramics and Concrete Manufacturing': 'Manufacturing',\n",
    "    'Chemical Manufacturing': 'Manufacturing',\n",
    "    'Mining': 'Miscellaneous',\n",
    "    'E-Learning Providers': 'Technology',\n",
    "    'Security and Investigations': 'Services',\n",
    "    'Translation and Localization': 'Services',\n",
    "    'Automation Machinery Manufacturing': 'Technology',\n",
    "    'Computer and Network Security': 'Technology',\n",
    "    'Political Organizations': 'Government and Public Policy',\n",
    "    'Environmental Services': 'Government and Public Policy',\n",
    "    'Oil and Gas': 'Miscellaneous',\n",
    "    'Real Estate': 'Real Estate and Construction',\n",
    "    'Think Tanks': 'Government and Public Policy',\n",
    "    'Executive Offices': 'Miscellaneous',\n",
    "    'Law Practice': 'Services',\n",
    "    'Nanotechnology Research': 'Miscellaneous',\n",
    "    'International Trade and Development': 'Government and Public Policy',\n",
    "    'Personal Care Product Manufacturing': 'Manufacturing',\n",
    "    'Philanthropic Fundraising Services': 'Services',\n",
    "    'Entertainment Providers': 'Media and Entertainment',\n",
    "    'Market Research': 'Media and Entertainment',\n",
    "    'Movies, Videos, and Sound': 'Media and Entertainment',\n",
    "    'Sporting Goods Manufacturing': 'Manufacturing',\n",
    "    'Graphic Design': 'Services',\n",
    "    'Technology, Information and Internet': 'Technology',\n",
    "    'IT Services and IT Consulting': 'Technology',\n",
    "    'Retail Office Equipment': 'Retail and Consumer Goods',\n",
    "    'Wholesale Import and Export': 'Services',\n",
    "    'Capital Markets': 'Financial and Investment',\n",
    "    'Law Enforcement': 'Services',\n",
    "    'Freight and Package Transportation': 'Transportation and Logistics',\n",
    "    'Industrial Machinery Manufacturing': 'Manufacturing',\n",
    "    'Non-profit Organizations': 'Miscellaneous',\n",
    "    'Retail Art Supplies': 'Retail and Consumer Goods',\n",
    "    'Animation and Post-production': 'Media and Entertainment',\n",
    "    'Transportation, Logistics, Supply Chain and Storage': 'Transportation and Logistics',\n",
    "    'Aviation and Aerospace Component Manufacturing': 'Transportation and Logistics',\n",
    "    'Fundraising': 'Financial and Investment',\n",
    "    'Railroad Equipment Manufacturing': 'Transportation and Logistics',\n",
    "    'Construction': 'Real Estate and Construction',\n",
    "    'Investment Management': 'Financial and Investment',\n",
    "    'Utilities': 'Miscellaneous',\n",
    "    'Retail Luxury Goods and Jewelry': 'Retail and Consumer Goods',\n",
    "    'Warehousing and Storage': 'Transportation and Logistics',\n",
    "    'Media Production': 'Media and Entertainment',\n",
    "    'Gambling Facilities and Casinos': 'Media and Entertainment',\n",
    "    'Defense and Space Manufacturing': 'Manufacturing',\n",
    "    'Facilities Services': 'Services',\n",
    "    'Government Relations Services': 'Government and Public Policy',\n",
    "    'Advertising Services': 'Media and Entertainment',\n",
    "    'Paper and Forest Product Manufacturing': 'Manufacturing',\n",
    "    'Packaging and Containers Manufacturing': 'Manufacturing',\n",
    "    'Telecommunications': 'Technology',\n",
    "    'Medical Equipment Manufacturing': 'Healthcare and Medical',\n",
    "    'Beverage Manufacturing': 'Manufacturing',\n",
    "    'Restaurants': 'Retail and Consumer Goods',\n",
    "    'Leasing Non-residential Real Estate': 'Real Estate and Construction',\n",
    "    'Newspaper Publishing': 'Media and Entertainment',\n",
    "    'Armed Forces': 'Miscellaneous',\n",
    "    'Appliances, Electrical, and Electronics Manufacturing': 'Manufacturing',\n",
    "    'Hospitality': 'Services',\n",
    "    'Pharmaceutical Manufacturing': 'Healthcare and Medical',\n",
    "    'Research Services': 'Services',\n",
    "    'Retail Apparel and Fashion': 'Retail and Consumer Goods',\n",
    "    'Photography': 'Media and Entertainment',\n",
    "    'Wellness and Fitness Services': 'Services',\n",
    "    'Truck Transportation': 'Transportation and Logistics',\n",
    "    'Consumer Services': 'Services',\n",
    "    'Wholesale Building Materials': 'Services',\n",
    "    'Human Resources Services': 'Services',\n",
    "    'Airlines and Aviation': 'Transportation and Logistics',\n",
    "    'Machinery Manufacturing': 'Manufacturing',\n",
    "    'Individual and Family Services': 'Services',\n",
    "    'Motor Vehicle Manufacturing': 'Manufacturing',\n",
    "    'Performing Arts': 'Media and Entertainment',\n",
    "    'Museums, Historical Sites, and Zoos': 'Media and Entertainment',\n",
    "    'Broadcast Media Production and Distribution': 'Media and Entertainment',\n",
    "    'Banking': 'Financial and Investment',\n",
    "    'Recreational Facilities': 'Miscellaneous',\n",
    "    'Government Administration': 'Government and Public Policy',\n",
    "    'Public Relations and Communications Services': 'Media and Entertainment',\n",
    "    'Fisheries': 'Miscellaneous',\n",
    "    'Medical Practices': 'Healthcare and Medical',\n",
    "    'Religious Institutions': 'Miscellaneous',\n",
    "    'Online Audio and Video Media': 'Media and Entertainment',\n",
    "    'Artists and Writers': 'Miscellaneous',\n",
    "    'Biotechnology Research': 'Healthcare and Medical',\n",
    "    'Legal Services': 'Services',\n",
    "    'Retail': 'Retail and Consumer Goods',\n",
    "    'Civil Engineering': 'Services',\n",
    "    'Libraries': 'Miscellaneous',\n",
    "    'Alternative Dispute Resolution': 'Miscellaneous',\n",
    "    'Manufacturing': 'Miscellaneous',\n",
    "    'Design Services': 'Services',\n",
    "    'Dairy Product Manufacturing': 'Manufacturing',\n",
    "    'Higher Education': 'Education and Training',\n",
    "    'Civic and Social Organizations': 'Miscellaneous',\n",
    "    'Textile Manufacturing': 'Manufacturing',\n",
    "    'Venture Capital and Private Equity Principals': 'Financial and Investment',\n",
    "    'Mental Health Care': 'Healthcare and Medical',\n",
    "    'Musicians': 'Media and Entertainment',\n",
    "    'Farming': 'Miscellaneous',\n",
    "    'Computer Games': 'Media and Entertainment',\n",
    "    'Strategic Management Services': 'Services',\n",
    "    'Food and Beverage Manufacturing': 'Manufacturing',\n",
    "    'Primary and Secondary Education': 'Education and Training',\n",
    "    'Alternative Medicine': 'Healthcare and Medical',\n",
    "    'Legislative Offices': 'Services',\n",
    "    'Administration of Justice': 'Services',\n",
    "    'Mobile Gaming Apps': 'Media and Entertainment'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40cab6cc-d6ce-401b-a206-302903d7e81f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "meta_industry = udf(lambda x: meta_industries_12[x])\n",
    "companies = companies.filter(companies.industries.isNotNull())\n",
    "companies = companies.withColumn('meta_industry', meta_industry(col('industries')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32c5348b-330a-4a02-acfe-68609ef6fd62",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "filterBlob": "{\"filterGroups\":[],\"syncTimestamp\":1735032540993}",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "companies.select(\"industries\", \"meta_industry\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fad66422-3168-4381-99fd-e981bbf04d4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 1: Data processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "550f3d42-3a1e-4e55-a47d-064de63181eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col, when, split\n",
    "\n",
    "companies_exploded = companies.withColumn(\"similar_exploded\", explode(col(\"similar\"))) \\\n",
    "    .withColumn(\"similar_company_name\", col(\"similar_exploded.title\")) \\\n",
    "    .withColumn(\"similar_company_link\", split(col(\"similar_exploded.links\"), r\"\\?\")[0]) \\\n",
    "    .drop(\"similar_exploded\", \"similar\")\n",
    "\n",
    "# Create the new DataFrame with company_A_name and company_B_name\n",
    "companies_ordered = companies_exploded.withColumn(\n",
    "    \"company_A_name\", when(col(\"name\") < col(\"similar_company_name\"), col(\"name\")).otherwise(col(\"similar_company_name\"))\n",
    ").withColumn(\n",
    "    \"company_B_name\", when(col(\"name\") < col(\"similar_company_name\"), col(\"similar_company_name\")).otherwise(col(\"name\"))\n",
    ").withColumn(\n",
    "    \"company_A_url\", when(col(\"name\") < col(\"similar_company_name\"), col(\"url\")).otherwise(col(\"similar_company_link\"))\n",
    ").withColumn(\n",
    "    \"company_B_url\", when(col(\"name\") < col(\"similar_company_name\"), col(\"similar_company_link\")).otherwise(col(\"url\"))\n",
    ").select(\"company_A_name\", \"company_B_name\", \"company_A_url\", \"company_B_url\").dropDuplicates() \\\n",
    "    .filter(col(\"company_A_name\") != col(\"company_B_name\"))\n",
    "\n",
    "companies_ordered.cache()\n",
    "filtered_companies = companies.drop(\"similar\")\n",
    "\n",
    "companies_A_ordered = companies_ordered.join(\n",
    "    filtered_companies,\n",
    "    ((col(\"company_A_name\") == col(\"name\")) & (col(\"company_A_url\") == col(\"url\"))),\n",
    "    \"inner\"\n",
    ").drop(\"company_A_name\", \"company_A_url\")\n",
    "\n",
    "companies_A_ordered.cache()\n",
    "\n",
    "def rename_columns(df, rename_func, letter):\n",
    "    for column in df.columns:\n",
    "        df = df.withColumnRenamed(column, rename_func(column, letter))\n",
    "    return df\n",
    "\n",
    "def change_col_name(name, letter):\n",
    "    # Add prefix \"company_A_\" only if the column name doesn't start with \"company_A\" or \"company_B\"\n",
    "    if not name.startswith(\"company_A\") and not name.startswith(\"company_B\"):\n",
    "        return f\"company_{letter}_{name}\"\n",
    "    return name\n",
    "\n",
    "companies_A_ordered = rename_columns(companies_A_ordered, change_col_name, \"A\")\n",
    "\n",
    "companies_B_ordered = companies_A_ordered.join(\n",
    "    filtered_companies,\n",
    "    ((col(\"company_B_name\") == col(\"name\")) & (col(\"company_B_url\") == col(\"url\"))),\n",
    "    \"inner\"\n",
    ").drop(\"company_B_name\", \"company_B_url\")\n",
    "companies_B_ordered.cache()\n",
    "\n",
    "companies_AB_ordered = rename_columns(companies_B_ordered, change_col_name, \"B\")\n",
    "companies_AB_ordered.cache()\n",
    "companies_AB_ordered.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "143fc371-acf5-4c55-bda4-a208c097b401",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the list of company_A_names to filter\n",
    "filter_list = [\n",
    "    'Altium Leadership',\n",
    "    'American College of Music - ACM Online',\n",
    "    'Arieca Inc.',\n",
    "    'Bango Bowls',\n",
    "    'Blue Llamas, Inc.'\n",
    "]\n",
    "\n",
    "# Filter the DataFrame and order by company_A_name and company_B_name\n",
    "filtered_df = companies_AB_ordered.filter(col(\"company_A_name\").isin(filter_list)) \\\n",
    "                                  .orderBy([\"company_A_name\", \"company_B_name\"])\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "display(filtered_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b697c485-fe2d-4b9c-a032-f19db36b247c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 2: Missingness Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24304afe-446b-4ba0-9ab8-42d8129dd497",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count, expr, lit\n",
    "from pyspark.sql.types import ArrayType, StructType, StringType, LongType, IntegerType, FloatType, DoubleType\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F\n",
    "import seaborn as sns\n",
    "\n",
    "companies = spark.read.parquet('/dbfs/linkedin_train_data')\n",
    "\n",
    "# Define null-like values\n",
    "null_identifiers = [\"unknown\", \"[]\", \"None\", \"null\", \"\"]\n",
    "\n",
    "# Detect missing values for each column\n",
    "missing_stats = []\n",
    "total_rows = companies.count()\n",
    "\n",
    "for col_name in companies.columns:\n",
    "    # Get the data type of the column\n",
    "    col_type = dict(companies.dtypes)[col_name]\n",
    "    \n",
    "    if isinstance(companies.schema[col_name].dataType, StringType):  # String columns\n",
    "        missing_count = companies.filter(\n",
    "            (col(col_name).isNull()) | (col(col_name).isin(null_identifiers))\n",
    "        ).count()\n",
    "    elif isinstance(companies.schema[col_name].dataType, ArrayType):  # Array columns\n",
    "        missing_count = companies.filter(col(col_name).isNull() | (col(col_name) == lit([]))).count()\n",
    "    elif isinstance(companies.schema[col_name].dataType, StructType):  # Struct columns\n",
    "        missing_count = companies.filter(col(col_name).isNull()).count()\n",
    "    else:  # Numeric or other simple types\n",
    "        missing_count = companies.filter(col(col_name).isNull()).count()\n",
    "    \n",
    "    missing_percentage = (missing_count / total_rows) * 100\n",
    "    missing_stats.append((col_name, missing_count, missing_percentage))\n",
    "\n",
    "# Create a DataFrame for missing stats\n",
    "missing_df = spark.createDataFrame(missing_stats, [\"Column\", \"Missing_Count\", \"Missing_Percentage\"])\n",
    "\n",
    "# Filter columns with missing percentage at least 2% and less than 95%\n",
    "columns_to_analyze = missing_df.filter(\n",
    "    (col(\"Missing_Percentage\") >= 2) & (col(\"Missing_Percentage\") < 95)\n",
    ")\n",
    "\n",
    "summary_df = columns_to_analyze\n",
    "\n",
    "# Show the summary table\n",
    "summary_df.display()\n",
    "\n",
    "# Save the summary as a CSV file\n",
    "summary_df.write.csv(\"/dbfs/missing_values_summary\", header=True, mode=\"overwrite\")\n",
    "\n",
    "# Visualization (convert to Pandas for external plotting)\n",
    "summary_pd = summary_df.toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Missing_Percentage', y='Column', data=summary_pd.sort_values(by='Missing_Percentage'))\n",
    "plt.title('Missing Data Distribution')\n",
    "plt.xlabel('Missing Percentage')\n",
    "plt.ylabel('Columns')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "filtered_columns = summary_df.select(\"Column\").rdd.flatMap(lambda x: x).collect()\n",
    "filtered_companies = companies.select(*filtered_columns)\n",
    "\n",
    "# Calculating correlation of missing values\n",
    "missing_correlation = filtered_companies.select([F.col(c).isNull().cast(\"int\").alias(c) for c in filtered_columns])\n",
    "missing_correlation_df = missing_correlation.toPandas()\n",
    "correlation_matrix = missing_correlation_df.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", center=0, fmt=\".1f\")\n",
    "plt.title(\"Correlation Heatmap of Missing Values\", fontsize=16)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b16e4dd-f74f-49a4-a0ad-8a1a5f7fea17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 3: Evaluating Similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48837b27-06f6-49c5-86b5-794d341c5131",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# using the df from question 1 here the good and bad companies matches\n",
    "\n",
    "#GOOD\n",
    "print(\"2 GOOD matches:\")\n",
    "good_filtered_AB_companies = companies_AB_ordered.filter((col(\"company_A_industries\") == col(\"company_B_industries\")) \n",
    "                                                         & (col(\"company_A_meta_industry\") == col(\"company_B_meta_industry\"))\n",
    "                                                         & (col(\"company_A_company_size\") == col(\"company_B_company_size\"))) \\\n",
    "                                    .orderBy([\"company_A_name\", \"company_B_name\"])\n",
    "good_filtered_AB_companies.limit(2).display()\n",
    "\n",
    "# BAD\n",
    "print(\"2 BAD matches:\")\n",
    "bad_filtered_AB_companies = companies_AB_ordered.filter((col(\"company_A_industries\") != col(\"company_B_industries\")) \n",
    "                                                        & (col(\"company_A_type\") != col(\"company_B_type\"))\n",
    "                                                        & (col(\"company_A_meta_industry\") != col(\"company_B_meta_industry\")) \n",
    "                                                        & (col(\"company_A_company_size\") != col(\"company_B_company_size\"))) \\\n",
    "                                .orderBy([\"company_A_name\", \"company_B_name\"])\n",
    "bad_filtered_AB_companies.limit(2).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a02c393-d613-4526-a4d0-62e2d015cd82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# more from the dfs so we can obsarve the matches\n",
    "print(\"GOOD matches:\")\n",
    "good_filtered_AB_companies.limit(10).display()\n",
    "print(\"BAD matches:\")\n",
    "bad_filtered_AB_companies.limit(10).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f0a0a0b-039a-4cea-903b-8a424930bf04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "paired_companies = companies_AB_ordered\n",
    "\n",
    "# Calculate percentage of pairs with matching industries\n",
    "industry_match_rate = paired_companies.filter(\n",
    "    col(\"company_A_industries\") == col(\"company_B_industries\")\n",
    ").count() / paired_companies.count()\n",
    "\n",
    "print(f\"Industry Match Rate: {industry_match_rate * 100:.2f}%\")\n",
    "\n",
    "# Calculate percentage of pairs with matching types\n",
    "type_match_rate = paired_companies.filter(\n",
    "    col(\"company_A_type\") == col(\"company_B_type\")\n",
    ").count() / paired_companies.count()\n",
    "\n",
    "print(f\"Type Match Rate: {type_match_rate * 100:.2f}%\")\n",
    "\n",
    "# Calculate percentage of pairs with matching companies sizes\n",
    "size_match_rate = paired_companies.filter(\n",
    "    col(\"company_A_company_size\") == col(\"company_B_company_size\")\n",
    ").count() / paired_companies.count()\n",
    "\n",
    "print(f\"Company size Match Rate: {size_match_rate * 100:.2f}%\")\n",
    "\n",
    "# Check overlap in specialties\n",
    "from pyspark.sql.functions import split, array_intersect, size\n",
    "\n",
    "# Tokenize specialties into keywords\n",
    "paired_companies = paired_companies.withColumn(\n",
    "    \"specialties_A_keywords\", split(col(\"company_A_specialties\"), \", \")\n",
    ").withColumn(\n",
    "    \"specialties_B_keywords\", split(col(\"company_B_specialties\"), \", \")\n",
    ")\n",
    "\n",
    "# Calculate overlap\n",
    "paired_companies = paired_companies.withColumn(\n",
    "    \"specialties_overlap\",\n",
    "    size(array_intersect(col(\"specialties_A_keywords\"), col(\"specialties_B_keywords\")))\n",
    ")\n",
    "\n",
    "# Calculate overlap rate\n",
    "specialty_match = paired_companies.filter(col(\"specialties_overlap\") > 0)\n",
    "specialty_match_rate = specialty_match.count() / paired_companies.count()\n",
    "\n",
    "print(f\"Specialty Match Rate: {specialty_match_rate * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdd731f9-942f-4b51-a3c2-3311e9495469",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, CountVectorizer, StopWordsRemover, IDF\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Replace nulls with empty strings to ensure proper text processing\n",
    "companies_df = companies.fillna({\n",
    "    \"about\": \"\",\n",
    "    \"industries\": \"\",\n",
    "    \"type\": \"\",\n",
    "    \"specialties\": \"\"\n",
    "})\n",
    "\n",
    "# Tokenize \"about\", specialties\", \"type\", and \"industries\" columns\n",
    "tokenizer = Tokenizer(inputCol=\"about\", outputCol=\"about_words\")\n",
    "companies_df = tokenizer.transform(companies_df)\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"specialties\", outputCol=\"specialties_words\")\n",
    "companies_df = tokenizer.transform(companies_df)\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"industries\", outputCol=\"industries_words\")\n",
    "companies_df = tokenizer.transform(companies_df)\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"type\", outputCol=\"type_words\")\n",
    "companies_df = tokenizer.transform(companies_df)\n",
    "\n",
    "# Remove stop words from the columns\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"about_words\", outputCol=\"about_clean\")\n",
    "companies_df = stop_words_remover.transform(companies_df)\n",
    "\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"specialties_words\", outputCol=\"specialties_clean\")\n",
    "companies_df = stop_words_remover.transform(companies_df)\n",
    "\n",
    "count_vectorizer = CountVectorizer(inputCol=\"about_clean\", outputCol=\"about_features\", vocabSize=500)\n",
    "companies_df = count_vectorizer.fit(companies_df).transform(companies_df)\n",
    "\n",
    "count_vectorizer = CountVectorizer(inputCol=\"specialties_clean\", outputCol=\"specialties_features\", vocabSize=500)\n",
    "companies_df = count_vectorizer.fit(companies_df).transform(companies_df)\n",
    "\n",
    "count_vectorizer = CountVectorizer(inputCol=\"industries_words\", outputCol=\"industries_features\", vocabSize=500)\n",
    "companies_df = count_vectorizer.fit(companies_df).transform(companies_df)\n",
    "\n",
    "count_vectorizer = CountVectorizer(inputCol=\"type_words\", outputCol=\"type_features\", vocabSize=500)\n",
    "companies_df = count_vectorizer.fit(companies_df).transform(companies_df)\n",
    "\n",
    "# Create the final features vector\n",
    "companies_with_features = companies_df\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"about_features\", \"specialties_features\", \"type_features\", \"industries_features\"],\n",
    "    outputCol=\"features_vector\"\n",
    ")\n",
    "\n",
    "companies_with_features = assembler.transform(companies_with_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ec02237-d2af-48bd-a18a-7017028d793d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans(k=5, seed=1, featuresCol=\"features_vector\", predictionCol=\"cluster\")\n",
    "kmeans_model = kmeans.fit(companies_with_features)\n",
    "clustered_df = kmeans_model.transform(companies_with_features)\n",
    "\n",
    "# Add cluster information for company_A\n",
    "companies_AB = companies_AB_ordered.join(\n",
    "    clustered_df.select(col(\"name\").alias(\"company_A\"), col(\"cluster\").alias(\"cluster_A\")),\n",
    "    (col(\"company_A_name\") == col(\"company_A\")),\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "# Add cluster information for company_B\n",
    "companies_AB = companies_AB.join(\n",
    "    clustered_df.select(col(\"name\").alias(\"company_B\"), col(\"cluster\").alias(\"cluster_B\")),\n",
    "    (col(\"company_B_name\") == col(\"company_B\")),\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "# Add a column indicating whether the clusters match\n",
    "companies_AB = companies_AB.withColumn(\n",
    "    \"same_cluster\",\n",
    "    col(\"cluster_A\") == col(\"cluster_B\")\n",
    ")\n",
    "\n",
    "# Count matches and mismatches\n",
    "validation_results = companies_AB.groupBy(\"same_cluster\").count()\n",
    "validation_results.show()\n",
    "\n",
    "\n",
    "# Filter true and false cases\n",
    "tp = companies_AB.filter((col(\"same_cluster\") == True)).count()\n",
    "fn = companies_AB.filter((col(\"same_cluster\") == False)).count()\n",
    "fp = companies_AB.filter(\n",
    "    (col(\"same_cluster\") == True) & (col(\"cluster_A\").isNotNull()) & (col(\"cluster_B\").isNotNull())\n",
    ").count() - tp\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"Precision: {precision}, Recall: {recall}, F1 Score: {f1_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fed1b1fe-8750-4b47-b680-f3095c2f923f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 4: Supervised Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55d2726f-ee46-4b32-a64d-ad97d11a71d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "companies_test = spark.read.parquet('/dbfs/linkedin_companies_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14aa57f7-a691-4429-a475-0818da9b5be5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, size, when, count, lit\n",
    "\n",
    "# Selecting relevant columns\n",
    "relevant_columns = [\n",
    "    \"id\", \"about\", \"affiliated\", \"company_size\", \"country_code\", \"employees\",\n",
    "    \"employees_in_linkedin\", \"followers\", \"founded\", \"funding\",\n",
    "    \"headquarters\", \"name\", \"slogan\", \"specialties\", \"type\", \"updates\"\n",
    "]\n",
    "train_data = companies.select(relevant_columns + [\"industries\", \"meta_industry\"]).dropna(subset=[\"industries\", \"meta_industry\"])\n",
    "test_data = companies_test.select(relevant_columns)\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "# Calculating missing data percentage for each column:\n",
    "\n",
    "total_rows = train_data.count()\n",
    "def calculate_missing_percentage(df, columns):\n",
    "    missing_percentages = {}\n",
    "    \n",
    "    for column in columns:\n",
    "        # Handle array columns\n",
    "        if isinstance(df.schema[column].dataType, ArrayType):\n",
    "            missing_count = df.filter(size(col(column)) == 0).count()\n",
    "        # Handle string or other non-array columns\n",
    "        elif isinstance(df.schema[column].dataType, StringType):\n",
    "            missing_count = df.filter(col(column).isNull() | (col(column) == \"\")).count()\n",
    "        # Handle numeric or other columns\n",
    "        elif isinstance(df.schema[column].dataType, LongType) or isinstance(df.schema[column].dataType, IntegerType):\n",
    "            missing_count = df.filter(col(column).isNull()).count()\n",
    "        else:\n",
    "            missing_count = df.filter(col(column).isNull()).count()\n",
    "        \n",
    "        missing_percentage = (missing_count / total_rows) * 100\n",
    "        missing_percentages[column] = missing_percentage\n",
    "        \n",
    "    return missing_percentages\n",
    "\n",
    "# Calculate missing percentages\n",
    "missing_percentages = calculate_missing_percentage(train_data, relevant_columns)\n",
    "\n",
    "# Display results\n",
    "for column, percentage in missing_percentages.items():\n",
    "    print(f\"Column: {column}, Missing Data Percentage: {percentage:.2f}%\")\n",
    "\n",
    "# The results printed:\n",
    "Column: about, Missing Data Percentage: 71.19%\n",
    "Column: affiliated, Missing Data Percentage: 99.20%\n",
    "Column: company_size, Missing Data Percentage: 7.09%\n",
    "Column: country_code, Missing Data Percentage: 0.00%\n",
    "Column: employees, Missing Data Percentage: 70.22%\n",
    "Column: employees_in_linkedin, Missing Data Percentage: 70.79%\n",
    "Column: followers, Missing Data Percentage: 0.00%\n",
    "Column: founded, Missing Data Percentage: 0.00%\n",
    "Column: funding, Missing Data Percentage: 99.49%\n",
    "Column: headquarters, Missing Data Percentage: 0.22%\n",
    "Column: name, Missing Data Percentage: 0.00%\n",
    "Column: slogan, Missing Data Percentage: 86.25%\n",
    "Column: specialties, Missing Data Percentage: 87.41%\n",
    "Column: type, Missing Data Percentage: 78.29%\n",
    "Column: updates, Missing Data Percentage: 93.17%\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93a5891a-1855-4273-8575-721d7bcc1bd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, current_date, when, size, split, array_contains, expr, explode, avg\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler, Word2Vec, Tokenizer, StopWordsRemover\n",
    "from pyspark.ml import Pipeline\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Remove nulls for string and numeric columns\n",
    "train_data_no_nulls = train_data.na.fill({\n",
    "    \"about\": \"\", 'company_size': 'Unknown', 'country_code': 'Unknown',\n",
    "    \"employees_in_linkedin\": 0, \"followers\": 0, \"founded\": 0,\n",
    "    \"headquarters\": \"\", \"name\": \"\", \"slogan\": \"\", \"specialties\": \"\", \"type\": \"\"\n",
    "})\n",
    "test_data_no_nulls = test_data.na.fill({\n",
    "    \"about\": \"\", 'company_size': 'Unknown', 'country_code': 'Unknown',\n",
    "    \"employees_in_linkedin\": 0, \"followers\": 0, \"founded\": 0,\n",
    "    \"headquarters\": \"\", \"name\": \"\", \"slogan\": \"\", \"specialties\": \"\", \"type\": \"\"\n",
    "})\n",
    "\n",
    "# Remove nulls for array columns\n",
    "train_data_no_nulls = train_data_no_nulls.withColumn(\"updates\", when(col(\"updates\").isNull(), lit([])).otherwise(col(\"updates\"))) \\\n",
    "    .withColumn(\"employees\", when(col(\"employees\").isNull(), lit([])).otherwise(col(\"employees\"))) \\\n",
    "    .withColumn(\"affiliated\", when(col(\"affiliated\").isNull(), lit([])).otherwise(col(\"affiliated\")))\n",
    "test_data_no_nulls = test_data_no_nulls.withColumn(\"updates\", when(col(\"updates\").isNull(), lit([])).otherwise(col(\"updates\"))) \\\n",
    "    .withColumn(\"employees\", when(col(\"employees\").isNull(), lit([])).otherwise(col(\"employees\"))) \\\n",
    "    .withColumn(\"affiliated\", when(col(\"affiliated\").isNull(), lit([])).otherwise(col(\"affiliated\")))\n",
    "\n",
    "\n",
    "\"\"\" Feature Engineering \"\"\"\n",
    "\n",
    "# Company size indexing\n",
    "size_map_expr = expr(\"\"\"\n",
    "    CASE\n",
    "        WHEN split(company_size, \" \")[0] = 'Unknown' THEN 0\n",
    "        WHEN split(company_size, \" \")[0] = '1' THEN 1\n",
    "        WHEN split(company_size, \" \")[0] = '2-10' THEN 2\n",
    "        WHEN split(company_size, \" \")[0] = '11-50' THEN 3\n",
    "        WHEN split(company_size, \" \")[0] = '51-200' THEN 4\n",
    "        WHEN split(company_size, \" \")[0] = '201-500' THEN 5\n",
    "        WHEN split(company_size, \" \")[0] = '501-1,000' THEN 6\n",
    "        WHEN split(company_size, \" \")[0] = '1,001-5,000' THEN 7\n",
    "        WHEN split(company_size, \" \")[0] = '5,001-10,000' THEN 8\n",
    "        WHEN split(company_size, \" \")[0] = '10,001+' THEN 9\n",
    "        ELSE 0\n",
    "    END\n",
    "\"\"\")\n",
    "train_data_company_size = train_data_no_nulls.withColumn(\"company_size_encoded\", size_map_expr).drop(\"company_size\")\n",
    "test_data_company_size = test_data_no_nulls.withColumn(\"company_size_encoded\", size_map_expr).drop(\"company_size\")\n",
    "\n",
    "# One-hot encoding of company size\n",
    "one_hot_encoder = OneHotEncoder(inputCol=\"company_size_encoded\", outputCol=\"company_size_one_hot\")\n",
    "one_hot_model = one_hot_encoder.fit(train_data_company_size)\n",
    "train_data_company_size = one_hot_model.transform(train_data_company_size).drop(\"company_size_encoded\")\n",
    "test_data_company_size = one_hot_model.transform(test_data_company_size).drop(\"company_size_encoded\")\n",
    "\n",
    "# Founding year encoding\n",
    "current_year = datetime.now().year\n",
    "train_data_founding = train_data_company_size.withColumn(\"num_years\", lit(current_year) - col(\"founded\")).drop(\"founded\").cache()\n",
    "test_data_founding = test_data_company_size.withColumn(\"num_years\", lit(current_year) - col(\"founded\")).drop(\"founded\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd13a350-385a-46b3-a660-ce1046dd02a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, Word2Vec\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "import numpy as np\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "\n",
    "# Specialties vectorization using Word2Vec\n",
    "specialties_tokenizer = Tokenizer(inputCol=\"specialties\", outputCol=\"specialties_tokens\")\n",
    "specialties_stopwords_remover = StopWordsRemover(inputCol=\"specialties_tokens\", outputCol=\"filtered_specialties\")\n",
    "specialties_word2vec = Word2Vec(\n",
    "    inputCol=\"filtered_specialties\", \n",
    "    outputCol=\"specialties_vector\", \n",
    "    vectorSize=50, \n",
    "    minCount=2\n",
    ")\n",
    "\n",
    "specialties_pipeline = Pipeline(stages=[specialties_tokenizer, specialties_stopwords_remover, specialties_word2vec])\n",
    "specialties_model = specialties_pipeline.fit(train_data_founding)\n",
    "train_data_specialties = specialties_model.transform(train_data_founding) \\\n",
    "    .drop(\"specialties\", \"specialties_tokens\", \"filtered_specialties\").cache()\n",
    "test_data_specialties = specialties_model.transform(test_data_founding) \\\n",
    "    .drop(\"specialties\", \"specialties_tokens\", \"filtered_specialties\").cache()\n",
    "\n",
    "\n",
    "# Description embeddings from \"about\", \"name\", \"slogan\"\n",
    "def create_text_pipeline(input_col, token_output_col, filtered_output_col):\n",
    "    # Define tokenizers and stopwords removers for each column\n",
    "    tokenizer = Tokenizer(inputCol=input_col, outputCol=token_output_col)\n",
    "    stopwords_remover = StopWordsRemover(inputCol=token_output_col, outputCol=filtered_output_col)\n",
    "    return tokenizer, stopwords_remover\n",
    "\n",
    "about_tokenizer, about_stopwords_remover = create_text_pipeline(\"about\", \"about_tokens\", \"filtered_about\")\n",
    "name_tokenizer, name_stopwords_remover = create_text_pipeline(\"name\", \"name_tokens\", \"filtered_name\")\n",
    "slogan_tokenizer, slogan_stopwords_remover = create_text_pipeline(\"slogan\", \"slogan_tokens\", \"filtered_slogan\")\n",
    "\n",
    "# Word2Vec models of each column\n",
    "about_word2vec = Word2Vec(inputCol=\"filtered_about\", outputCol=\"about_vector\", vectorSize=100, minCount=2)\n",
    "name_word2vec = Word2Vec(inputCol=\"filtered_name\", outputCol=\"name_vector\", vectorSize=100, minCount=2)\n",
    "slogan_word2vec = Word2Vec(inputCol=\"filtered_slogan\", outputCol=\"slogan_vector\", vectorSize=100, minCount=2)\n",
    "\n",
    "# Pipelines of each column\n",
    "about_pipeline = Pipeline(stages=[about_tokenizer, about_stopwords_remover, about_word2vec])\n",
    "name_pipeline = Pipeline(stages=[name_tokenizer, name_stopwords_remover, name_word2vec])\n",
    "slogan_pipeline = Pipeline(stages=[slogan_tokenizer, slogan_stopwords_remover, slogan_word2vec])\n",
    "\n",
    "# Fit the models to the data\n",
    "about_model = about_pipeline.fit(train_data_specialties)\n",
    "name_model = name_pipeline.fit(train_data_specialties)\n",
    "slogan_model = slogan_pipeline.fit(train_data_specialties)\n",
    "\n",
    "train_data_with_vectors = about_model.transform(train_data_specialties) \\\n",
    "    .drop(\"about\", \"about_tokens\", \"filtered_about\")\n",
    "test_data_with_vectors = about_model.transform(test_data_specialties) \\\n",
    "    .drop(\"about\", \"about_tokens\", \"filtered_about\")\n",
    "train_data_with_vectors = name_model.transform(train_data_with_vectors) \\\n",
    "    .drop(\"name\", \"name_tokens\", \"filtered_name\")\n",
    "test_data_with_vectors = name_model.transform(test_data_with_vectors) \\\n",
    "    .drop(\"name\", \"name_tokens\", \"filtered_name\")\n",
    "train_data_with_vectors = slogan_model.transform(train_data_with_vectors) \\\n",
    "    .drop(\"slogan\", \"slogan_tokens\", \"filtered_slogan\")\n",
    "test_data_with_vectors = slogan_model.transform(test_data_with_vectors) \\\n",
    "    .drop(\"slogan\", \"slogan_tokens\", \"filtered_slogan\")\n",
    "\n",
    "train_data_with_vectors.write.parquet(\"dbfs:/tmp/train_data_with_vectors.parquet\", mode=\"overwrite\")\n",
    "test_data_with_vectors.write.parquet(\"dbfs:/tmp/test_data_with_vectors.parquet\", mode=\"overwrite\")\n",
    "\n",
    "train_data_description = spark.read.parquet(\"dbfs:/tmp/train_data_with_vectors.parquet\")\n",
    "test_data_description = spark.read.parquet(\"dbfs:/tmp/test_data_with_vectors.parquet\")\n",
    "\n",
    "def average_vectors(about_vec, name_vec, slogan_vec):\n",
    "    about_vec = about_vec.toArray().tolist() if isinstance(about_vec, DenseVector) else about_vec\n",
    "    name_vec = name_vec.toArray().tolist() if isinstance(name_vec, DenseVector) else name_vec\n",
    "    slogan_vec = slogan_vec.toArray().tolist() if isinstance(slogan_vec, DenseVector) else slogan_vec\n",
    "    \n",
    "    # Filter out None vectors\n",
    "    vectors = [v for v in [about_vec, name_vec, slogan_vec] if v is not None]\n",
    "    if not vectors:\n",
    "        return None\n",
    "    \n",
    "    # Initialize an empty vector\n",
    "    vector_length = len(vectors[0])\n",
    "    avg_vector = [0.0] * vector_length\n",
    "    # Sum all vectors element-wise\n",
    "    for vec in vectors:\n",
    "        for i in range(vector_length):\n",
    "            avg_vector[i] += vec[i]\n",
    "    # Divide by the number of vectors\n",
    "    num_vectors = len(vectors)\n",
    "    avg_vector = [x / num_vectors for x in avg_vector]\n",
    "    return avg_vector\n",
    "\n",
    "average_udf = udf(average_vectors, ArrayType(FloatType()))\n",
    "\n",
    "train_data_description = train_data_description.withColumn(\n",
    "    \"description_vector\", average_udf(\"about_vector\", \"name_vector\", \"slogan_vector\")\n",
    ").drop(\"about_vector\", \"name_vector\", \"slogan_vector\")\n",
    "test_data_description = test_data_description.withColumn(\n",
    "    \"description_vector\", average_udf(\"about_vector\", \"name_vector\", \"slogan_vector\")\n",
    ").drop(\"about_vector\", \"name_vector\", \"slogan_vector\")\n",
    "\n",
    "# Use UDF to convert description_vector to DenseVector\n",
    "def array_to_vector(arr):\n",
    "    if arr is not None:\n",
    "        return Vectors.dense(arr)\n",
    "    return None\n",
    "array_to_vector_udf = udf(array_to_vector, VectorUDT())\n",
    "train_data_description = train_data_description.withColumn(\"description_vector\", array_to_vector_udf(\"description_vector\"))\n",
    "test_data_description = test_data_description.withColumn(\"description_vector\", array_to_vector_udf(\"description_vector\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f43e47a5-de3b-4bcc-a1af-5d1da5b892e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, when, size, col, array, lit, concat_ws\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType\n",
    "from pyspark.ml.feature import Word2Vec, Tokenizer, StringIndexer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Affiliated column processing\n",
    "def extract_subtitles(affiliated):\n",
    "    return [item['subtitle'] for item in affiliated if 'subtitle' in item and item['subtitle']]\n",
    "extract_subtitles_udf = udf(extract_subtitles, ArrayType(StringType()))\n",
    "\n",
    "# Extract non-null subtitles from the array of dictionaries\n",
    "train_data_affiliated = train_data_description.withColumn(\"affiliated_subtitles\", extract_subtitles_udf(train_data_description[\"affiliated\"])).drop(\"affiliated\")\n",
    "test_data_affiliated = test_data_description.withColumn(\"affiliated_subtitles\", extract_subtitles_udf(test_data_description[\"affiliated\"])).drop(\"affiliated\")\n",
    "\n",
    "# Cocatenating all subtitles into a single string\n",
    "train_data_affiliated = train_data_affiliated.withColumn(\"affiliated_subtitles\", \n",
    "    when(size(train_data_affiliated[\"affiliated_subtitles\"]) == 0, array(lit(\"missing\")))\n",
    "    .otherwise(train_data_affiliated[\"affiliated_subtitles\"]))\n",
    "train_data_affiliated = train_data_affiliated.withColumn(\"affiliated_subtitles_string\", concat_ws(\" \", train_data_affiliated[\"affiliated_subtitles\"])).drop(\"affiliated_subtitles\")\n",
    "\n",
    "test_data_affiliated = test_data_affiliated.withColumn(\"affiliated_subtitles\",\n",
    "    when(size(test_data_affiliated[\"affiliated_subtitles\"]) == 0, array(lit(\"missing\")))\n",
    "    .otherwise(test_data_affiliated[\"affiliated_subtitles\"]))\n",
    "test_data_affiliated = test_data_affiliated.withColumn(\"affiliated_subtitles_string\", concat_ws(\" \", test_data_affiliated[\"affiliated_subtitles\"])).drop(\"affiliated_subtitles\")\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"affiliated_subtitles_string\", outputCol=\"affiliated_words\")\n",
    "train_data_affiliated = tokenizer.transform(train_data_affiliated).drop(\"affiliated_subtitles_string\")\n",
    "test_data_affiliated = tokenizer.transform(test_data_affiliated).drop(\"affiliated_subtitles_string\")\n",
    "\n",
    "# Train a Word2Vec model on the tokenized words\n",
    "word2Vec = Word2Vec(vectorSize=100, minCount=0, inputCol=\"affiliated_words\", outputCol=\"affiliated_embeddings\")\n",
    "model = word2Vec.fit(train_data_affiliated)\n",
    "train_data_affiliated = model.transform(train_data_affiliated).drop(\"affiliated_words\")\n",
    "test_data_affiliated = model.transform(test_data_affiliated).drop(\"affiliated_words\")\n",
    "\n",
    "\n",
    "# Employees column processing\n",
    "train_data_employees = train_data_affiliated.withColumn(\"employee_subtitles\", extract_subtitles_udf(train_data_affiliated[\"employees\"])).drop(\"employees\")\n",
    "test_data_employees = test_data_affiliated.withColumn(\"employee_subtitles\", extract_subtitles_udf(test_data_affiliated[\"employees\"])).drop(\"employees\")\n",
    "\n",
    "# Cocatenating all subtitles into a single string\n",
    "train_data_employees = train_data_employees.withColumn(\"employee_subtitles\",\n",
    "    when(size(train_data_employees[\"employee_subtitles\"]) == 0, array(lit(\"missing\")))\n",
    "    .otherwise(train_data_employees[\"employee_subtitles\"]))\n",
    "train_data_employees = train_data_employees.withColumn(\"employee_subtitles_string\", concat_ws(\" \", train_data_employees[\"employee_subtitles\"])).drop(\"employee_subtitles\")\n",
    "\n",
    "test_data_employees = test_data_employees.withColumn(\"employee_subtitles\",\n",
    "    when(size(test_data_employees[\"employee_subtitles\"]) == 0, array(lit(\"missing\")))\n",
    "    .otherwise(test_data_employees[\"employee_subtitles\"]))\n",
    "test_data_employees = test_data_employees.withColumn(\"employee_subtitles_string\", concat_ws(\" \", test_data_employees[\"employee_subtitles\"])).drop(\"employee_subtitles\")\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"employee_subtitles_string\", outputCol=\"employee_words\") \n",
    "train_data_employees = tokenizer.transform(train_data_employees).drop(\"employee_subtitles_string\")\n",
    "test_data_employees = tokenizer.transform(test_data_employees).drop(\"employee_subtitles_string\")\n",
    "\n",
    "# Train the Word2Vec model on the tokenized words\n",
    "word2Vec = Word2Vec(vectorSize=100, minCount=0, inputCol=\"employee_words\", outputCol=\"employee_embeddings\")\n",
    "model = word2Vec.fit(train_data_employees)\n",
    "train_data_employees = model.transform(train_data_employees).drop(\"employee_words\")\n",
    "test_data_employees = model.transform(test_data_employees).drop(\"employee_words\")\n",
    "\n",
    "\n",
    "# Headquarters location\n",
    "location_indexer = StringIndexer(inputCol=\"headquarters\", outputCol=\"location_index\", handleInvalid=\"keep\")\n",
    "train_data_location = location_indexer.fit(train_data_employees).transform(train_data_employees) \\\n",
    "    .drop(\"headquarters\")\n",
    "test_data_location = location_indexer.fit(train_data_employees).transform(test_data_employees) \\\n",
    "    .drop(\"headquarters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f566ea1-2213-4c47-a47f-d85c87724647",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col, to_date, datediff, year, lit, udf, coalesce, struct, log1p\n",
    "from pyspark.ml import Pipeline\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "\n",
    "# Popularity calculation according to the number of comments and likes\n",
    "train_data_popularity = train_data_location.withColumn(\n",
    "    \"popularity\",\n",
    "    expr(\"aggregate(updates, cast(0 as BIGINT), (acc, x) -> acc + coalesce(x.comments_count, 0) + coalesce(x.likes_count, 0))\")\n",
    ").drop(\"updates\")\n",
    "test_data_popularity = test_data_location.withColumn(\n",
    "    \"popularity\", \n",
    "    expr(\"aggregate(updates, cast(0 as BIGINT), (acc, x) -> acc + coalesce(x.comments_count, 0) + coalesce(x.likes_count, 0))\")\n",
    ").drop(\"updates\")\n",
    "\n",
    "\n",
    "# Log-transform the number of followers\n",
    "train_data_followers = train_data_popularity.withColumn(\"followers_log\", log1p(col(\"followers\"))).drop(\"followers\")\n",
    "test_data_followers = test_data_popularity.withColumn(\"followers_log\", log1p(col(\"followers\"))).drop(\"followers\")\n",
    "\n",
    "\n",
    "# Organization type encoding\n",
    "train_data_org_types = train_data_followers.withColumn(\"type\", col(\"type\").cast(\"string\"))\n",
    "test_data_org_types = test_data_followers.withColumn(\"type\", col(\"type\").cast(\"string\"))\n",
    "\n",
    "organization_type_indexer = StringIndexer(inputCol=\"type\", outputCol=\"organization_type_index\", handleInvalid=\"keep\")\n",
    "types_model = organization_type_indexer.fit(train_data_org_types)\n",
    "train_data_org_types = types_model.transform(train_data_org_types).drop(\"type\")\n",
    "test_data_org_types = types_model.transform(test_data_org_types).drop(\"type\")\n",
    "\n",
    "\n",
    "# Funding column\n",
    "train_data_funding = train_data_org_types.withColumn(\n",
    "    \"funding\",\n",
    "    struct(\n",
    "        coalesce(col(\"funding.last_round_type\"), lit(\"Unknown\")).alias(\"last_round_type\"),  # Replace NULL with 'Unknown'\n",
    "        coalesce(col(\"funding.last_round_date\"), to_date(lit(\"1900-01-01\"), \"yyyy-MM-dd\")).alias(\"last_round_date\"),\n",
    "        coalesce(col(\"funding.rounds\"), lit(0)).alias(\"rounds\")\n",
    "    )\n",
    ")\n",
    "test_data_funding = test_data_org_types.withColumn(\n",
    "    \"funding\", \n",
    "    struct(\n",
    "        coalesce(col(\"funding.last_round_type\"), lit(\"Unknown\")).alias(\"last_round_type\"),  # Replace NULL with 'Unknown'\n",
    "        coalesce(col(\"funding.last_round_date\"), to_date(lit(\"1900-01-01\"), \"yyyy-MM-dd\")).alias(\"last_round_date\"),\n",
    "        coalesce(col(\"funding.rounds\"), lit(0)).alias(\"rounds\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Encode last round type\n",
    "train_data_funding = train_data_funding.withColumn(\"last_round_type\", col(\"funding.last_round_type\"))\n",
    "test_data_funding = test_data_funding.withColumn(\"last_round_type\", col(\"funding.last_round_type\"))\n",
    "\n",
    "round_type_indexer = StringIndexer(inputCol=\"last_round_type\", outputCol=\"last_round_type_index\", handleInvalid=\"keep\")\n",
    "round_type_model = round_type_indexer.fit(train_data_funding)\n",
    "train_data_funding = round_type_model.transform(train_data_funding).drop(\"last_round_type\")\n",
    "test_data_funding = round_type_model.transform(test_data_funding).drop(\"last_round_type\")\n",
    "\n",
    "# Calculate years since last funding round\n",
    "current_year = datetime.now().year\n",
    "train_data_funding = train_data_funding.withColumn(\"years_since_last_round\", \n",
    "    (current_year - year(col(\"funding.last_round_date\"))).cast(\"double\"))\n",
    "test_data_funding = test_data_funding.withColumn(\"years_since_last_round\", \n",
    "    (current_year - year(col(\"funding.last_round_date\"))).cast(\"double\"))\n",
    "\n",
    "# Use funding rounds as-is\n",
    "train_data_funding = train_data_funding.withColumn(\"funding_rounds\", col(\"funding.rounds\")).drop(\"funding\")\n",
    "test_data_funding = test_data_funding.withColumn(\"funding_rounds\", col(\"funding.rounds\")).drop(\"funding\")\n",
    "\n",
    "\n",
    "# Country code indexing\n",
    "country_code_indexer = StringIndexer(inputCol=\"country_code\", outputCol=\"country_code_index\", handleInvalid=\"keep\")\n",
    "train_data_country = country_code_indexer.fit(train_data_funding).transform(train_data_funding) \\\n",
    "    .drop(\"country_code\")\n",
    "test_data_country = country_code_indexer.fit(test_data_funding).transform(test_data_funding) \\\n",
    "    .drop(\"country_code\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d264cf86-80f7-4f9b-880a-d45197eb59b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "\n",
    "# Apply VectorAssembler\n",
    "vector_assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"country_code_index\", \"employees_in_linkedin\", \"followers_log\", \"location_index\",\n",
    "        \"company_size_one_hot\", \"num_years\", \"specialties_vector\", \"description_vector\",\n",
    "        \"affiliated_embeddings\", \"employee_embeddings\", \"popularity\", \n",
    "        \"organization_type_index\", \"last_round_type_index\", \"years_since_last_round\", \n",
    "        \"funding_rounds\"\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "train_transformed = vector_assembler.transform(train_data_country)\n",
    "test_transformed = vector_assembler.transform(test_data_country)\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"meta_industry\", outputCol=\"label_idx\")\n",
    "train_transformed = indexer.fit(train_transformed).transform(train_transformed)\n",
    "\n",
    "# Final dataset\n",
    "train_final = train_transformed.select(\"id\", \"features\", \"meta_industry\", \"label_idx\")\n",
    "test_final = test_transformed.select(\"id\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9313835d-3337-46e2-b761-a80b16aaf2d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Split data into training and val sets\n",
    "train, validation = train_final.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "# LogisticRegression classifier\n",
    "lr = LogisticRegression(labelCol=\"label_idx\", featuresCol=\"features\", maxIter=10, regParam=0.1)\n",
    "lr_model = lr.fit(train)\n",
    "\n",
    "# Make predictions on the validation dataset\n",
    "predictions = lr_model.transform(validation)\n",
    "\n",
    "# Evaluate the model, and compute weighted precision, recall, and F1 score\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label_idx\", predictionCol=\"prediction\")\n",
    "weighted_precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "weighted_recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "f1_score = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(f\"Weighted Precision: {weighted_precision:.3f}\")\n",
    "print(f\"Weighted Recall: {weighted_recall:.3f}\")\n",
    "print(f\"F1 Score: {f1_score:.3f}\")  # Best F1 Score on val: 0.503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cad39f40-8f5b-4d6e-bd1d-0b5af39277f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Plot to show the influence of every feature on the prediction of each class.\n",
    "\n",
    "# Retrieve class names (meta-industry) corresponding to each label\n",
    "class_mapping = train_final.select(\"label_idx\", \"meta_industry\").distinct().toPandas()\n",
    "class_mapping = class_mapping.sort_values(by=\"label_idx\")  # Ensure classes are sorted by label\n",
    "class_names = class_mapping[\"meta_industry\"].tolist()\n",
    "\n",
    "# Coefficient matrix\n",
    "coefficient_matrix = lr_model.coefficientMatrix\n",
    "\n",
    "# Convert it to a DataFrame\n",
    "coefficients_df = pd.DataFrame(\n",
    "    coefficient_matrix.toArray(),\n",
    "    columns=[\"Feature \" + str(i) for i in range(coefficient_matrix.numCols)]\n",
    ")\n",
    "coefficients_df[\"Class\"] = class_names  # Use class names instead of indices\n",
    "transposed_df = coefficients_df.set_index(\"Class\").transpose()\n",
    "\n",
    "# Plot the coefficients\n",
    "plt.figure(figsize=(12, 8))\n",
    "for class_name in class_names:\n",
    "    plt.plot(transposed_df.index, transposed_df[class_name], label=class_name)\n",
    "\n",
    "plt.title(\"Feature Coefficients by Meta-Industry Class (Logistic Regression)\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Coefficient Value\")\n",
    "plt.xticks(\n",
    "    ticks=range(0, len(transposed_df.index), max(1, len(transposed_df.index) // 20)),  # Show every 20th feature\n",
    "    labels=transposed_df.index[::max(1, len(transposed_df.index) // 20)],  # Corresponding labels\n",
    "    rotation=45,  # Rotate labels for readability\n",
    "    ha=\"right\"  # Align labels to the right\n",
    ")\n",
    "plt.legend(title=\"Meta-Industry\", loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b9f8745-7d01-4f17-a49f-6d7b493296ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Plot confusion matrix\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Create confusion matrix\n",
    "pred_labels = predictions.selectExpr(\"CAST(label_idx AS INT) AS label_idx\", \"CAST(prediction AS INT) AS prediction\")\n",
    "conf_matrix = pred_labels.groupBy(\"label_idx\", \"prediction\").count().toPandas()\n",
    "\n",
    "# Pivot the confusion matrix and replace NaN with 0\n",
    "conf_matrix_pivot = conf_matrix.pivot(index=\"label_idx\", columns=\"prediction\", values=\"count\").fillna(0).astype(int)\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix_pivot, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "841cfa8c-0644-491f-a6ed-384edd393868",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "# Retrieve class names (meta-industry) corresponding to each label\n",
    "class_mapping = train_final.select(\"label_idx\", \"meta_industry\").distinct()\n",
    "label_to_meta_industry = {row[\"label_idx\"]: row[\"meta_industry\"] for row in class_mapping.collect()}\n",
    "\n",
    "def get_meta_industry(label):\n",
    "    # Map label indices to meta-industry names\n",
    "    return label_to_meta_industry.get(label, \"Unknown\")\n",
    "meta_industry_udf = udf(get_meta_industry, StringType())\n",
    "\n",
    "# Predict on the test set\n",
    "test_predictions = lr_model.transform(test_final)\n",
    "\n",
    "final_results = test_predictions.withColumn(\"label\", meta_industry_udf(col(\"prediction\"))) \\\n",
    "                                .select(\"id\", \"label\")\n",
    "\n",
    "final_results.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddbe3246-d6e2-423d-87b1-3d0c817c3576",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Analysis Report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7afa0a53-f473-431e-9f13-ef717d45db4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Write here a textual summary of all the steps you implemented:\n",
    "- Feature engineering and selection methods \n",
    "- Preprocessing pipeline \n",
    "- Model selection\n",
    "- Error analysis and Class-wise Performance Metrics "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1851252059405131,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "HW2_322620873_207176462_314779166",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
